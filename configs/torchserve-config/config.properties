# Server configurations
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082

# Maximum number of workers per model
default_workers_per_model=4

# Response timeout in seconds
default_response_timeout=120

# Size of the job queue for inference requests
job_queue_size=100

# Maximum batch size for inference
batch_size=4
max_batch_delay=100

# Enable metrics API
metrics_mode=prometheus

# Model specific configurations
default_workers_per_model=1
model_store=/app/model_store

# Logging configurations
log_location=/app/logs/
number_of_netty_threads=32

# Python configurations for custom handler
python_multiprocessing=true