Failure # 1 (occurred at 2024-12-23_14-43-35)
[36mray::_Inner.train()[39m (pid=122018, ip=172.26.204.107, actor_id=732cec895a4a805ea28c725801000000, repr=TorchTrainer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/tune/trainable/trainable.py", line 331, in train
    raise skipped from exception_cause(skipped)
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/air/_internal/util.py", line 107, in run
    self._ret = self._target(*self._args, **self._kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/tune/trainable/function_trainable.py", line 45, in <lambda>
    training_func=lambda: self._trainable_func(self.config),
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/base_trainer.py", line 799, in _trainable_func
    super()._trainable_func(self._merged_config)
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/tune/trainable/function_trainable.py", line 250, in _trainable_func
    output = fn()
             ^^^^
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/base_trainer.py", line 107, in _train_coordinator_fn
    trainer.training_loop()
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/data_parallel_trainer.py", line 471, in training_loop
    self._run_training(training_iterator)
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/data_parallel_trainer.py", line 370, in _run_training
    for training_results in training_iterator:
                            ^^^^^^^^^^^^^^^^^
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/trainer.py", line 129, in __next__
    next_results = self._run_with_error_handling(self._fetch_next_result)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/trainer.py", line 94, in _run_with_error_handling
    return func()
           ^^^^^^
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/trainer.py", line 174, in _fetch_next_result
    results = self._backend_executor.get_next_results()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/_internal/backend_executor.py", line 606, in get_next_results
    results = self.get_with_failure_handling(futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/_internal/backend_executor.py", line 719, in get_with_failure_handling
    self._increment_failures()
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/_internal/backend_executor.py", line 781, in _increment_failures
    raise failure
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/_internal/utils.py", line 57, in check_for_failure
    ray.get(object_ref)
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.
	class_name: RayTrainWorker
	actor_id: f19713fc6483b5c4ff26736e01000000
	pid: 122200
	namespace: aa1ea2ee-576c-41fa-90f0-62e86eb8be65
	ip: 172.26.204.107
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
