Failure # 1 (occurred at 2024-12-23_15-07-54)
[36mray::_Inner.train()[39m (pid=150172, ip=172.26.204.107, actor_id=a35f3758a66253af8688c52f01000000, repr=TorchTrainer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/tune/trainable/trainable.py", line 331, in train
    raise skipped from exception_cause(skipped)
  File "/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/train/_internal/utils.py", line 57, in check_for_failure
    ray.get(object_ref)
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.26.204.107, ID: 4cec74f17515d48c7a209e37ae3cd840d3e7d15f896dba7b517bb688) where the task (actor ID: 2d3e50da24eb2d14c51ac45601000000, name=RayTrainWorker.__init__, pid=150367, memory used=2.13GB) was running was 7.60GB / 7.62GB (0.996817), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9b54e8c10495ef2524fb762dbe0d1b4df5df5b5d8407b733ef4f3071) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.26.204.107`. To see the logs of the worker, use `ray logs worker-9b54e8c10495ef2524fb762dbe0d1b4df5df5b5d8407b733ef4f3071*out -ip 172.26.204.107. Top 10 memory users:
PID	MEM(GB)	COMMAND
150367	2.13	ray::_RayTrainWorker__execute.get_next
133280	0.52	/home/fahad/study/kserving/.venv/bin/python3 model/src/model/train.py
8484	0.46	/home/fahad/.vscode-server/bin/fabdb6a30b49f79a7aba0f2ad9df9b399473380f/node /home/fahad/.vscode-ser...
150172	0.42	ray::_Inner.train
133483	0.29	/home/fahad/study/kserving/.venv/bin/python3 /home/fahad/study/kserving/.venv/lib/python3.12/site-pa...
133410	0.25	/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_...
133576	0.23	/home/fahad/study/kserving/.venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --rayle...
133481	0.15	/home/fahad/study/kserving/.venv/bin/python3 -u /home/fahad/study/kserving/.venv/lib/python3.12/site...
133577	0.12	/home/fahad/study/kserving/.venv/bin/python3 -u /home/fahad/study/kserving/.venv/lib/python3.12/site...
6962	0.10	/home/fahad/.vscode-server/bin/fabdb6a30b49f79a7aba0f2ad9df9b399473380f/node --dns-result-order=ipv4...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
